#!/usr/bin/env bash
set -euo pipefail

VERSION="0.3.6"

CONFIG_PATH="${BAKKER_CONFIG:-}"
BAKKER_API_URL=""
REQUEST_TIMEOUT=""
DEFAULT_OUTPUT=""
DEFAULT_CONFIRM_IMPORT=""
DEFAULT_PROFILE=""
VERBOSE_LEVEL=0

OVERRIDE_API_URL=""
OVERRIDE_TIMEOUT=""
OVERRIDE_OUTPUT=""
OVERRIDE_PROFILE=""

PROFILE_NAMES=""

usage() {
  cat <<'USAGE'
Usage:
  bakker [global options] <command> [options]
  bakker help [command]

Global options:
  --config <path>        Path to config.toml (default: ./bakker.config.toml, then ~/.config/bakker/bakker.config.toml)
  --api-url <url>        Override Bakker API URL
  --timeout <seconds>    Override API timeout
  --output <table|json>  Override output format
  --profile <name>       Default destination profile for import
  -v|-vv|-vvv            Progress detail for import/download commands
  --help                 Show this help

Commands:
  backup list [--db <name>] [--latest] [--json]                List backups from Bakker API (with IDs)
  backup download [--output <path>|--stdout] [--force] <id|filename>  Download backup archive only
  import [--profile <name>] [--yes] <id|filename>              Import backup into destination DB
  profiles list                                                 List configured destination profiles
  profiles show <name>                                          Show one destination profile
  doctor                                                        Run local and Bakker health checks
  version                                                       Print CLI version

Password env convention:
  For profile "<name>", set "<NAME>_DB_PASS" (uppercased, non-alnum -> _).
  Example: profile "local-dev" -> LOCAL_DEV_DB_PASS

Examples:
  bakker --help
  bakker help import
  bakker backup --help
  bakker backup download --help
  bakker import --help
USAGE
}

usage_backups() {
  cat <<'USAGE'
Usage:
  bakker backup list [--db <name>] [--latest] [--json]
  bakker backup download [--output <path>|--stdout] [--force] <id|filename>

List options:
  --db <name>    Filter backups to a Bakker database config name
  --latest       Show only latest backup per database (or for --db)
  --json         Print raw API JSON instead of table output
  --help, -h     Show this help

Download options:
  --output <path>  Write archive to file path (default: ./<filename>)
  --stdout         Stream archive bytes to stdout
  --force          Overwrite output file if it already exists
  -v|-vv|-vvv      Richer and more frequent download progress updates
  --help, -h       Show this help
USAGE
}

usage_backup_download() {
  cat <<'USAGE'
Usage:
  bakker backup download [--output <path>|--stdout] [--force] <id|filename>

Options:
  --output <path>  Write archive to file path (default: ./<filename>)
  --stdout         Stream archive bytes to stdout
  --force          Overwrite output file if it already exists
  -v|-vv|-vvv      Richer and more frequent download progress updates
  --help, -h       Show this help

Arguments:
  <id|filename>    Backup ID from 'bakker backup list', or exact remote filename

Notes:
  - Auth token comes from BAKKER_AUTH_TOKEN or interactive prompt.
  - This command only downloads; it does not import into SQL.
USAGE
}

usage_import() {
  cat <<'USAGE'
Usage:
  bakker import [--profile <name>] [--yes] [--skip-connectivity-check] [--no-drop] <id|filename>

Options:
  --profile <name>  Destination profile from config.toml (or [defaults].profile)
  --yes             Skip import confirmation prompt
  --skip-connectivity-check
                    Skip MySQL auth preflight; keep a fast socket reachability check
  --no-drop         Keep existing destination database objects; import over current state
  -v|-vv|-vvv       Progress levels: heartbeat only, +stream %, +table tracker
  --help, -h        Show this help

Arguments:
  <id|filename>     Backup ID from 'bakker backup list', or local .sql.gz file

Notes:
  - For ID-based imports, Bakker API token comes from BAKKER_AUTH_TOKEN or interactive prompt.
  - Destination DB password lookup order: <PROFILE>_DB_PASS env var, then profile password in config, then interactive prompt.
  - By default, import drops and recreates the destination database first (disable with --no-drop).
  - Supported import format: .sql.gz.
  - Imports execute locally and require mysql + gunzip on this machine.
  - ID imports stream directly from Bakker API to mysql (no local temp file).
USAGE
}

usage_profiles() {
  cat <<'USAGE'
Usage:
  bakker profiles list
  bakker profiles show <name>

Options:
  --help, -h    Show this help
USAGE
}

usage_doctor() {
  cat <<'USAGE'
Usage:
  bakker doctor

Runs local and Bakker connectivity checks.

Options:
  --help, -h    Show this help
USAGE
}

command_help() {
  local topic="${1:-}"
  case "$topic" in
    ""|-h|--help) usage ;;
    backup|backups) usage_backups ;;
    backup-download|download) usage_backup_download ;;
    import) usage_import ;;
    profiles) usage_profiles ;;
    doctor) usage_doctor ;;
    version)
      cat <<'USAGE'
Usage:
  bakker version
USAGE
      ;;
    *)
      echo "Error: unknown help topic '$topic'" >&2
      usage >&2
      exit 2
      ;;
  esac
}

trim() {
  local s="$1"
  s="${s#"${s%%[![:space:]]*}"}"
  s="${s%"${s##*[![:space:]]}"}"
  printf '%s' "$s"
}

strip_inline_comment() {
  local s="$1"
  local out=""
  local i ch
  local in_single="false"
  local in_double="false"

  for ((i = 0; i < ${#s}; i++)); do
    ch="${s:i:1}"
    if [[ "$ch" == "'" && "$in_double" == "false" ]]; then
      if [[ "$in_single" == "true" ]]; then
        in_single="false"
      else
        in_single="true"
      fi
      out+="$ch"
      continue
    fi
    if [[ "$ch" == '"' && "$in_single" == "false" ]]; then
      if [[ "$in_double" == "true" ]]; then
        in_double="false"
      else
        in_double="true"
      fi
      out+="$ch"
      continue
    fi
    if [[ "$ch" == "#" && "$in_single" == "false" && "$in_double" == "false" ]]; then
      break
    fi
    out+="$ch"
  done

  printf '%s' "$out"
}

strip_quotes() {
  local v
  v="$(trim "$1")"
  if [[ "$v" =~ ^\"(.*)\"$ ]]; then
    printf '%s' "${BASH_REMATCH[1]}"
    return
  fi
  if [[ "$v" =~ ^\'(.*)\'$ ]]; then
    printf '%s' "${BASH_REMATCH[1]}"
    return
  fi
  printf '%s' "$v"
}

increase_verbosity() {
  local amount="${1:-1}"
  VERBOSE_LEVEL=$((VERBOSE_LEVEL + amount))
  if [[ "$VERBOSE_LEVEL" -gt 3 ]]; then
    VERBOSE_LEVEL=3
  fi
}

import_progress_interval_seconds() {
  case "$VERBOSE_LEVEL" in
    0) echo 0 ;;
    1) echo 15 ;;
    2) echo 10 ;;
    *) echo 3 ;;
  esac
}

download_progress_interval_seconds() {
  case "$VERBOSE_LEVEL" in
    0) echo 0 ;;
    1) echo 10 ;;
    2) echo 5 ;;
    *) echo 2 ;;
  esac
}

log_verbose() {
  local level="$1"
  shift
  if [[ "$VERBOSE_LEVEL" -ge "$level" ]]; then
    echo "$@" >&2
  fi
}

dd_supports_progress() {
  dd --help 2>&1 | grep -q 'status=LEVEL'
}

read_stream_progress_bytes() {
  local progress_file="$1"
  if [[ ! -s "$progress_file" ]]; then
    return 0
  fi
  awk '
    /bytes/ {
      raw=$1
      gsub(/[^0-9]/, "", raw)
      if (raw ~ /^[0-9]+$/) {
        value=raw
      }
    }
    END {
      if (value != "") {
        printf "%s", value
      }
    }
  ' "$progress_file"
}

now_millis() {
  local ms
  ms="$(date +%s%3N 2>/dev/null || true)"
  if [[ "$ms" =~ ^[0-9]+$ ]]; then
    printf '%s' "$ms"
    return
  fi
  printf '%s000' "$(date +%s)"
}

elapsed_millis() {
  local start_ms="$1"
  local end_ms="${2:-}"
  if [[ -z "$end_ms" ]]; then
    end_ms="$(now_millis)"
  fi
  if [[ ! "$start_ms" =~ ^[0-9]+$ || ! "$end_ms" =~ ^[0-9]+$ ]]; then
    printf '0'
    return
  fi
  if [[ "$end_ms" -lt "$start_ms" ]]; then
    printf '0'
    return
  fi
  printf '%s' "$((end_ms - start_ms))"
}

format_mib_per_second() {
  local bytes="$1"
  local duration_ms="$2"
  if [[ ! "$bytes" =~ ^[0-9]+$ || ! "$duration_ms" =~ ^[0-9]+$ || "$duration_ms" -le 0 ]]; then
    printf 'n/a'
    return
  fi
  awk -v b="$bytes" -v ms="$duration_ms" 'BEGIN { printf "%.2f", (b / 1048576) / (ms / 1000) }'
}

print_import_perf_summary() {
  local mode="$1"
  local resolve_ms="$2"
  local connectivity_ms="$3"
  local stream_load_ms="$4"
  local total_ms="$5"
  local source_size_bytes="$6"
  local throughput_text=""
  local mibps
  mibps="$(format_mib_per_second "$source_size_bytes" "$stream_load_ms")"
  if [[ "$mibps" != "n/a" ]]; then
    throughput_text=", throughput ${mibps} MiB/s"
  fi
  printf 'Import performance (%s): resolve %sms, connectivity %sms, stream+load %sms, total %sms%s\n' \
    "$mode" "$resolve_ms" "$connectivity_ms" "$stream_load_ms" "$total_ms" "$throughput_text" >&2
}

profile_key() {
  local name="$1"
  local key="$2"
  printf 'PROFILE__%s__%s' "$name" "$key"
}

set_profile_value() {
  local name="$1"
  local key="$2"
  local value="$3"
  local var
  var="$(profile_key "$name" "$key")"
  printf -v "$var" '%s' "$value"
}

get_profile_value() {
  local name="$1"
  local key="$2"
  local var
  var="$(profile_key "$name" "$key")"
  printf '%s' "${!var-}"
}

add_profile_name() {
  local name="$1"
  if [[ -z "$PROFILE_NAMES" ]]; then
    PROFILE_NAMES="$name"
    return
  fi
  local existing
  for existing in $PROFILE_NAMES; do
    if [[ "$existing" == "$name" ]]; then
      return
    fi
  done
  PROFILE_NAMES="$PROFILE_NAMES $name"
}

ensure_config_exists() {
  local cwd_config="./bakker.config.toml"
  local home_config="$HOME/.config/bakker/bakker.config.toml"

  if [[ -n "$CONFIG_PATH" ]]; then
    if [[ -f "$CONFIG_PATH" ]]; then
      return 0
    fi
    echo "Error: config file not found at $CONFIG_PATH" >&2
    echo "Create it first (see cli/README.md for an example)." >&2
    exit 3
  fi

  if [[ -f "$cwd_config" ]]; then
    CONFIG_PATH="$cwd_config"
    return 0
  fi

  if [[ -f "$home_config" ]]; then
    CONFIG_PATH="$home_config"
    return 0
  fi

  echo "Error: config file not found." >&2
  echo "Checked: $cwd_config" >&2
  echo "Checked: $home_config" >&2
  echo "Create one first (see cli/README.md for an example)." >&2
  exit 3
}

load_config() {
  ensure_config_exists

  local section=""
  local line raw key value profile_name

  while IFS= read -r raw || [[ -n "$raw" ]]; do
    line="$(strip_inline_comment "$raw")"
    line="$(trim "$line")"
    [[ -z "$line" ]] && continue

    if [[ "$line" =~ ^\[(.+)\]$ ]]; then
      section="${BASH_REMATCH[1]}"
      continue
    fi

    if [[ ! "$line" =~ ^([A-Za-z0-9_.-]+)[[:space:]]*=[[:space:]]*(.+)$ ]]; then
      continue
    fi

    key="${BASH_REMATCH[1]}"
    value="$(strip_quotes "${BASH_REMATCH[2]}")"

    case "$section" in
      bakker)
        case "$key" in
          api_url) BAKKER_API_URL="$value" ;;
          timeout_seconds) REQUEST_TIMEOUT="$value" ;;
        esac
        ;;
      defaults)
        case "$key" in
          output) DEFAULT_OUTPUT="$value" ;;
          confirm_import) DEFAULT_CONFIRM_IMPORT="$value" ;;
          profile) DEFAULT_PROFILE="$value" ;;
        esac
        ;;
      profiles.*)
        profile_name="${section#profiles.}"
        add_profile_name "$profile_name"
        case "$key" in
          host|port|user|database|password)
            set_profile_value "$profile_name" "$key" "$value"
            ;;
        esac
        ;;
    esac
  done < "$CONFIG_PATH"

  [[ -z "$BAKKER_API_URL" ]] && BAKKER_API_URL="http://localhost:3500"
  [[ -z "$REQUEST_TIMEOUT" ]] && REQUEST_TIMEOUT="30"
  [[ -z "$DEFAULT_OUTPUT" ]] && DEFAULT_OUTPUT="table"
  [[ -z "$DEFAULT_CONFIRM_IMPORT" ]] && DEFAULT_CONFIRM_IMPORT="true"

  if [[ -n "$OVERRIDE_API_URL" ]]; then
    BAKKER_API_URL="$OVERRIDE_API_URL"
  fi
  if [[ -n "$OVERRIDE_TIMEOUT" ]]; then
    REQUEST_TIMEOUT="$OVERRIDE_TIMEOUT"
  fi
  if [[ -n "$OVERRIDE_OUTPUT" ]]; then
    DEFAULT_OUTPUT="$OVERRIDE_OUTPUT"
  fi
  if [[ -n "$OVERRIDE_PROFILE" ]]; then
    DEFAULT_PROFILE="$OVERRIDE_PROFILE"
  fi

  return 0
}

require_cmd() {
  local cmd="$1"
  command -v "$cmd" >/dev/null 2>&1 || {
    echo "Error: required command not found: $cmd" >&2
    exit 3
  }
}

require_runtime() {
  require_cmd curl
  require_cmd jq
}

get_auth_token() {
  local token
  if [[ -n "${BAKKER_AUTH_TOKEN:-}" ]]; then
    token="$BAKKER_AUTH_TOKEN"
  elif [[ -t 0 ]]; then
    read -r -s -p "Enter Bakker API token: " token
    echo
  else
    echo "Error: BAKKER_AUTH_TOKEN is not set and no interactive TTY is available." >&2
    exit 3
  fi

  token="$(trim "$token")"
  # Remove common terminal paste wrappers/control chars that can break auth headers.
  token="${token//$'\e[200~'/}"
  token="${token//$'\e[201~'/}"
  token="$(printf '%s' "$token" | tr -d '\r\n' | tr -d '[:cntrl:]')"
  token="$(trim "$token")"

  # Strip optional surrounding quotes if pasted as a quoted value.
  if [[ "$token" =~ ^\"(.*)\"$ ]]; then
    token="${BASH_REMATCH[1]}"
  elif [[ "$token" =~ ^\'(.*)\'$ ]]; then
    token="${BASH_REMATCH[1]}"
  fi

  if [[ "$token" == [Bb]earer\ * ]]; then
    token="${token#* }"
  fi

  if [[ -z "$token" ]]; then
    echo "Error: token is required." >&2
    exit 3
  fi

  printf '%s' "$token"
}

normalize_api_url() {
  local url="$1"
  url="$(trim "$url")"
  url="${url%/}"
  if [[ "$url" == */api ]]; then
    url="${url%/api}"
  fi
  printf '%s' "$url"
}

api_request() {
  local method="$1"
  local path="$2"
  local token="$3"
  local tmp_file status

  tmp_file="$(mktemp)"
  local request_url
  request_url="$(normalize_api_url "$BAKKER_API_URL")$path"

  status="$(curl -sS -o "$tmp_file" -w '%{http_code}' --max-time "$REQUEST_TIMEOUT" \
    -H "Authorization: Bearer $token" \
    -X "$method" \
    "$request_url")" || {
      rm -f "$tmp_file"
      echo "Error: request failed: $method $request_url" >&2
      exit 4
    }

  if [[ "$status" -ge 400 ]]; then
    echo "Error: API returned HTTP $status for $method $path" >&2
    echo "Request URL: $request_url" >&2
    if [[ "$status" -eq 400 ]]; then
      echo "Hint: verify [bakker].api_url in config and token format." >&2
      echo "Hint: try exporting BAKKER_AUTH_TOKEN to avoid hidden paste chars in interactive input." >&2
    fi
    cat "$tmp_file" >&2
    rm -f "$tmp_file"
    exit 4
  fi

  cat "$tmp_file"
  rm -f "$tmp_file"
}

url_encode() {
  local raw="$1"
  local length i c encoded=""
  length=${#raw}
  for ((i = 0; i < length; i++)); do
    c="${raw:i:1}"
    case "$c" in
      [a-zA-Z0-9.~_-]) encoded+="$c" ;;
      *)
        printf -v hex '%02X' "'$c"
        encoded+="%$hex"
        ;;
    esac
  done
  printf '%s' "$encoded"
}

print_backups_table() {
  local json_data="$1"
  local filter_db="$2"
  local latest_only="$3"
  local latest_flag="false"
  if [[ "$latest_only" == "true" ]]; then
    latest_flag="true"
  fi

  while IFS=$'\t' read -r col1 col2 col3 col4 col5; do
    printf '%-5s %-48s %-19s %-20s %s\n' "$col1" "$col2" "$col3" "$col4" "$col5"
  done < <(
    {
      echo -e "ID\tNAME\tDATE\tDB\tSIZE"
      printf '%s' "$json_data" \
        | jq -r '
            to_entries[]
            | .key as $db
            | (.value // [])[]
            | [
                (
                  if (.id | tonumber?) == null then "-"
                  elif (.id | tonumber) > 0 then ((.id | tonumber) | tostring)
                  else "-"
                  end
                ),
                (.filename // ""),
                (.date // ""),
                $db,
                (
                  if (.sizeHuman // "") != "" then .sizeHuman
                  elif (.sizeMB // "") != "" then (.sizeMB | tostring)
                  else ""
                  end
                )
              ]
            | @tsv
        '
    } \
      | awk -F'\t' -v latest="$latest_flag" -v db_filter="$filter_db" '
          NR == 1 { print; next }
          db_filter != "" && $4 != db_filter { next }
          { print }
        ' \
      | {
          IFS= read -r header
          printf '%s\n' "$header"
          sort -t$'\t' -k4,4 -k3,3r \
            | awk -F'\t' -v latest="$latest_flag" '
                latest != "true" { print; next }
                !seen[$4]++ { print }
              '
        }
  )
}

resolve_backup_file_by_id() {
  local json_data="$1"
  local backup_id="$2"
  printf '%s' "$json_data" \
    | jq -r --argjson id "$backup_id" '
        [
          to_entries[]
          | .key as $db
          | (.value // [])[]
          | select((.id | tonumber?) == $id)
          | {
              file: (.filename // ""),
              db: $db,
              size: (
                if (.size | tonumber?) == null then ""
                else ((.size | tonumber | floor) | tostring)
                end
              )
            }
        ][0] as $row
        | if $row == null or ($row.file // "") == "" then
            empty
          else
            "\($row.file)\t\($row.db)\t\($row.size)"
          end
      '
}

resolve_backup_size_by_filename() {
  local json_data="$1"
  local filename="$2"
  printf '%s' "$json_data" \
    | jq -r --arg f "$filename" '
        [
          to_entries[]
          | (.value // [])[]
          | select((.filename // "") == $f)
          | .size
          | tonumber?
          | select(. != null and . > 0)
          | floor
          | tostring
        ][0] // ""
      '
}

resolve_server_backup_by_id() {
  local backup_id="$1"
  local token="$2"
  local filename=""
  local selected_db=""
  local selected_size_bytes=""
  local backup_meta_json backups_json available_ids
  local resolve_tmp resolve_status request_url

  resolve_tmp="$(mktemp)"
  request_url="$(normalize_api_url "$BAKKER_API_URL")/api/backups/by-id/$backup_id"
  resolve_status="$(curl -sS -o "$resolve_tmp" -w '%{http_code}' --max-time "$REQUEST_TIMEOUT" \
    -H "Authorization: Bearer $token" \
    "$request_url")" || {
      rm -f "$resolve_tmp"
      echo "Error: request failed: GET $request_url" >&2
      exit 4
    }

  if [[ "$resolve_status" -eq 404 ]]; then
    rm -f "$resolve_tmp"
    backups_json="$(api_request GET "/api/backups" "$token")"
    local resolved_backup_row
    resolved_backup_row="$(resolve_backup_file_by_id "$backups_json" "$backup_id" || true)"
    if [[ -n "$resolved_backup_row" ]]; then
      printf '%s\n' "$resolved_backup_row"
      return 0
    fi
    echo "Error: no backup found for ID '$backup_id'." >&2
    echo "Tip: run 'bakker --config $CONFIG_PATH backup list' and choose an ID from the first column." >&2
    available_ids="$(print_available_server_backup_ids "$backups_json" || true)"
    if [[ -n "$available_ids" ]]; then
      echo "Server IDs currently available: $available_ids" >&2
    else
      echo "The server response did not contain usable backup IDs." >&2
    fi
    exit 4
  fi

  if [[ "$resolve_status" -ge 400 ]]; then
    echo "Error: API returned HTTP $resolve_status for GET /api/backups/by-id/$backup_id" >&2
    cat "$resolve_tmp" >&2
    rm -f "$resolve_tmp"
    exit 4
  fi

  backup_meta_json="$(cat "$resolve_tmp")"
  rm -f "$resolve_tmp"

  filename="$(json_get_string_field "$backup_meta_json" "filename")"
  selected_db="$(json_get_string_field "$backup_meta_json" "database")"
  selected_size_bytes="$(json_get_number_field "$backup_meta_json" "size")"

  if [[ -z "$filename" ]]; then
    echo "Error: backup ID '$backup_id' resolved to an empty filename." >&2
    exit 4
  fi

  printf '%s\t%s\t%s\n' "$filename" "$selected_db" "$selected_size_bytes"
}

print_available_server_backup_ids() {
  local json_data="$1"
  printf '%s' "$json_data" \
    | jq -r '
        [
          to_entries[]
          | (.value // [])[]
          | .id
          | tonumber?
          | select(. != null and . > 0)
        ]
        | unique
        | sort
        | if length == 0 then empty else map(tostring) | join(", ") end
      '
}

require_profile() {
  local profile="$1"
  local found="false"
  local p
  for p in $PROFILE_NAMES; do
    if [[ "$p" == "$profile" ]]; then
      found="true"
      break
    fi
  done

  if [[ "$found" != "true" ]]; then
    echo "Error: profile '$profile' not found in $CONFIG_PATH" >&2
    exit 3
  fi

  local host port user database
  host="$(get_profile_value "$profile" host)"
  port="$(get_profile_value "$profile" port)"
  user="$(get_profile_value "$profile" user)"
  database="$(get_profile_value "$profile" database)"

  if [[ -z "$host" ]]; then
    echo "Error: profile '$profile' missing host" >&2
    exit 3
  fi
  if [[ -z "$port" ]]; then
    echo "Error: profile '$profile' missing port" >&2
    exit 3
  fi
  if [[ -z "$user" ]]; then
    echo "Error: profile '$profile' missing user" >&2
    exit 3
  fi
  if [[ -z "$database" ]]; then
    echo "Error: profile '$profile' missing database" >&2
    exit 3
  fi

  return 0
}

command_backups_list() {
  local filter_db=""
  local latest_only="false"
  local output_mode="$DEFAULT_OUTPUT"

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --db)
        [[ $# -lt 2 ]] && { echo "Error: --db requires a value" >&2; exit 2; }
        filter_db="$2"
        shift 2
        ;;
      --latest)
        latest_only="true"
        shift
        ;;
      --json)
        output_mode="json"
        shift
        ;;
      --help|-h)
        usage_backups
        return 0
        ;;
      *)
        echo "Error: unknown option for backup list: $1" >&2
        exit 2
        ;;
    esac
  done

  local token json_data
  token="$(get_auth_token)"
  json_data="$(api_request GET "/api/backups" "$token")"

  if [[ "$output_mode" == "json" ]]; then
    printf '%s\n' "$json_data"
    return 0
  fi

  print_backups_table "$json_data" "$filter_db" "$latest_only"
}

command_backups_download() {
  local target=""
  local output_path=""
  local write_stdout="false"
  local force_overwrite="false"
  local expected_size_bytes=""

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --output)
        [[ $# -lt 2 ]] && { echo "Error: --output requires a value" >&2; exit 2; }
        output_path="$2"
        shift 2
        ;;
      --stdout)
        write_stdout="true"
        shift
        ;;
      --force)
        force_overwrite="true"
        shift
        ;;
      -v|--verbose)
        increase_verbosity 1
        shift
        ;;
      -vv)
        increase_verbosity 2
        shift
        ;;
      -vvv)
        increase_verbosity 3
        shift
        ;;
      --help|-h)
        usage_backup_download
        return 0
        ;;
      -*)
        echo "Error: unknown option for backup download: $1" >&2
        exit 2
        ;;
      *)
        if [[ -n "$target" ]]; then
          echo "Error: backup download accepts exactly one target (<id|filename>)." >&2
          exit 2
        fi
        target="$1"
        shift
        ;;
    esac
  done

  if [[ -z "$target" ]]; then
    echo "Error: backup download requires an <id|filename> argument." >&2
    exit 2
  fi
  if [[ "$write_stdout" == "true" && -n "$output_path" ]]; then
    echo "Error: --stdout and --output cannot be used together." >&2
    exit 2
  fi

  local token filename selected_db
  token="$(get_auth_token)"
  if [[ "$target" =~ ^[0-9]+$ ]]; then
    if [[ "$target" -lt 1 ]]; then
      echo "Error: <id> must be a positive integer from 'bakker backup list'." >&2
      exit 2
    fi
    IFS=$'\t' read -r filename selected_db expected_size_bytes < <(resolve_server_backup_by_id "$target" "$token")
    echo "Resolved backup ID $target to '$filename' (database '$selected_db')."
  else
    filename="$target"
  fi

  if [[ "$filename" == */* || "$filename" == *\\* ]]; then
    echo "Error: invalid remote filename '$filename'." >&2
    exit 2
  fi
  if [[ "$filename" != *.sql.gz ]]; then
    echo "Error: unsupported backup format '$filename' (expected .sql.gz)." >&2
    exit 2
  fi

  if [[ ! "$expected_size_bytes" =~ ^[0-9]+$ || "$expected_size_bytes" -le 0 ]]; then
    expected_size_bytes=""
  fi
  if [[ -z "$expected_size_bytes" && "$VERBOSE_LEVEL" -ge 2 ]]; then
    local backups_json
    backups_json="$(api_request GET "/api/backups" "$token")"
    expected_size_bytes="$(resolve_backup_size_by_filename "$backups_json" "$filename" || true)"
    if [[ ! "$expected_size_bytes" =~ ^[0-9]+$ || "$expected_size_bytes" -le 0 ]]; then
      expected_size_bytes=""
    fi
  fi

  local encoded_filename download_url
  encoded_filename="$(url_encode "$filename")"
  download_url="$(normalize_api_url "$BAKKER_API_URL")/api/backups/$encoded_filename"
  local download_timeout_args=()
  if [[ "$REQUEST_TIMEOUT" != "0" && "$REQUEST_TIMEOUT" != "0.0" ]]; then
    download_timeout_args=(--connect-timeout "$REQUEST_TIMEOUT")
  fi

  if [[ "$write_stdout" != "true" ]]; then
    if [[ -z "$output_path" ]]; then
      output_path="./$filename"
    elif [[ -d "$output_path" ]]; then
      output_path="${output_path%/}/$filename"
    fi

    local output_dir
    output_dir="$(dirname "$output_path")"
    mkdir -p "$output_dir"

    if [[ -e "$output_path" && "$force_overwrite" != "true" ]]; then
      echo "Error: output file exists: $output_path (use --force to overwrite)." >&2
      exit 4
    fi
  fi

  local progress_interval monitor_pid download_pid download_status
  local progress_tmp_dir="" stream_progress_file=""
  local track_stream_bytes="false"
  local download_interrupted="false"
  local download_started_at download_started_ms transfer_ms downloaded_bytes transferred_bytes mibps
  download_started_at="$(date +%s)"
  download_started_ms="$(now_millis)"
  progress_interval="$(download_progress_interval_seconds)"

  if [[ "$VERBOSE_LEVEL" -ge 2 && -n "$expected_size_bytes" ]]; then
    if dd_supports_progress; then
      track_stream_bytes="true"
    else
      log_verbose 1 "Detailed download percentage disabled: your 'dd' does not support status=progress."
    fi
  fi
  if [[ "$track_stream_bytes" == "true" ]]; then
    progress_tmp_dir="$(mktemp -d)"
    stream_progress_file="$progress_tmp_dir/stream-progress.log"
    : >"$stream_progress_file"
  fi

  if [[ "$write_stdout" == "true" ]]; then
    echo "Streaming backup '$filename' to stdout..." >&2
  else
    echo "Downloading backup '$filename' to '$output_path'..." >&2
  fi
  if [[ "$VERBOSE_LEVEL" -ge 1 ]]; then
    echo "Download progress updates enabled (every ${progress_interval}s)." >&2
    log_verbose 1 "Verbose levels: -v heartbeat only, -vv adds stream percentage, -vvv adds live throughput."
  fi

  trap '
    download_interrupted="true"
    echo "Download interrupted; stopping active transfer..." >&2
    if [[ -n "${monitor_pid:-}" ]]; then
      kill "$monitor_pid" >/dev/null 2>&1 || true
    fi
    if [[ -n "${download_pid:-}" ]]; then
      if command -v pkill >/dev/null 2>&1; then
        pkill -TERM -P "$download_pid" >/dev/null 2>&1 || true
      fi
      kill "$download_pid" >/dev/null 2>&1 || true
    fi
  ' INT TERM

  if [[ "$write_stdout" == "true" ]]; then
    (
      if [[ "$track_stream_bytes" == "true" ]]; then
        curl -sS --fail-with-body "${download_timeout_args[@]}" \
          -H "Authorization: Bearer $token" \
          "$download_url" \
          | tee >(dd of=/dev/null status=progress 2>"$stream_progress_file")
      else
        curl -sS --fail-with-body "${download_timeout_args[@]}" \
          -H "Authorization: Bearer $token" \
          "$download_url"
      fi
    ) &
    download_pid=$!
  else
    (
      if [[ "$track_stream_bytes" == "true" ]]; then
        curl -sS --fail-with-body "${download_timeout_args[@]}" \
          -H "Authorization: Bearer $token" \
          "$download_url" \
          | tee >(dd of=/dev/null status=progress 2>"$stream_progress_file") \
          > "$output_path"
      else
        curl -sS --fail-with-body "${download_timeout_args[@]}" \
          -H "Authorization: Bearer $token" \
          "$download_url" \
          > "$output_path"
      fi
    ) &
    download_pid=$!
  fi

  monitor_pid=""
  if [[ "$VERBOSE_LEVEL" -ge 1 && "$progress_interval" -gt 0 ]]; then
    (
      previous_transferred_bytes=""
      stalled_seconds=0
      while kill -0 "$download_pid" >/dev/null 2>&1; do
        sleep "$progress_interval"
        if ! kill -0 "$download_pid" >/dev/null 2>&1; then
          break
        fi
        local_now="$(date +%s)"
        elapsed_seconds=$((local_now - download_started_at))
        details=""
        if [[ "$track_stream_bytes" == "true" && -n "$expected_size_bytes" && "$expected_size_bytes" -gt 0 ]]; then
          transferred_bytes="$(read_stream_progress_bytes "$stream_progress_file" || true)"
          if [[ "$transferred_bytes" =~ ^[0-9]+$ ]]; then
            percent=$((transferred_bytes * 100 / expected_size_bytes))
            if [[ "$percent" -gt 100 ]]; then
              percent=100
            fi
            transferred_mib=$((transferred_bytes / 1024 / 1024))
            total_mib=$((expected_size_bytes / 1024 / 1024))
            details="stream ${percent}% (${transferred_mib}/${total_mib} MiB)"
            if [[ -n "$previous_transferred_bytes" && "$transferred_bytes" -eq "$previous_transferred_bytes" ]]; then
              stalled_seconds=$((stalled_seconds + progress_interval))
            else
              stalled_seconds=0
            fi
            previous_transferred_bytes="$transferred_bytes"
            if [[ "$VERBOSE_LEVEL" -ge 3 ]]; then
              elapsed_ms_live="$(elapsed_millis "$download_started_ms")"
              live_mibps="$(format_mib_per_second "$transferred_bytes" "$elapsed_ms_live")"
              if [[ "$live_mibps" != "n/a" ]]; then
                details="$details, ${live_mibps} MiB/s"
              fi
            fi
            if [[ "$stalled_seconds" -ge $((progress_interval * 2)) ]]; then
              details="$details, no stream movement for ${stalled_seconds}s"
            fi
          fi
        fi
        if [[ -n "$details" ]]; then
          echo "Download in progress (${elapsed_seconds}s elapsed, $details)..." >&2
        else
          echo "Download in progress (${elapsed_seconds}s elapsed)..." >&2
        fi
      done
    ) &
    monitor_pid=$!
  fi

  set +e
  wait "$download_pid"
  download_status=$?
  set -e

  if [[ -n "$monitor_pid" ]]; then
    kill "$monitor_pid" >/dev/null 2>&1 || true
    wait "$monitor_pid" >/dev/null 2>&1 || true
  fi
  trap - INT TERM

  if [[ "$download_interrupted" == "true" ]]; then
    if [[ "$write_stdout" != "true" && -n "$output_path" ]]; then
      rm -f "$output_path"
    fi
    if [[ -n "$progress_tmp_dir" && -d "$progress_tmp_dir" ]]; then
      rm -rf "$progress_tmp_dir"
    fi
    echo "Download cancelled." >&2
    exit 130
  fi

  if [[ "$download_status" -ne 0 ]]; then
    if [[ "$write_stdout" != "true" && -n "$output_path" ]]; then
      rm -f "$output_path"
    fi
    if [[ -n "$progress_tmp_dir" && -d "$progress_tmp_dir" ]]; then
      rm -rf "$progress_tmp_dir"
    fi
    echo "Error: download failed." >&2
    exit "$download_status"
  fi

  transfer_ms="$(elapsed_millis "$download_started_ms")"
  downloaded_bytes=""
  if [[ "$write_stdout" == "true" ]]; then
    if [[ "$track_stream_bytes" == "true" ]]; then
      downloaded_bytes="$(read_stream_progress_bytes "$stream_progress_file" || true)"
    elif [[ -n "$expected_size_bytes" ]]; then
      downloaded_bytes="$expected_size_bytes"
    fi
  elif [[ -f "$output_path" ]]; then
    downloaded_bytes="$(wc -c < "$output_path" | tr -d '[:space:]')"
  fi
  mibps="$(format_mib_per_second "$downloaded_bytes" "$transfer_ms")"

  if [[ -n "$progress_tmp_dir" && -d "$progress_tmp_dir" ]]; then
    rm -rf "$progress_tmp_dir"
  fi

  if [[ "$write_stdout" == "true" ]]; then
    echo "Download completed for '$filename' (${downloaded_bytes:-unknown} bytes in ${transfer_ms}ms, ${mibps} MiB/s)." >&2
  else
    echo "Downloaded '$filename' to '$output_path' (${downloaded_bytes:-unknown} bytes in ${transfer_ms}ms, ${mibps} MiB/s)."
  fi
}

confirm_if_needed() {
  local prompt="$1"
  local no_confirm="$2"

  if [[ "$no_confirm" == "true" ]]; then
    return 0
  fi

  if [[ "$DEFAULT_CONFIRM_IMPORT" == "false" ]]; then
    return 0
  fi

  local answer
  read -r -p "$prompt [y/N]: " answer
  case "$answer" in
    y|Y|yes|YES) return 0 ;;
    *)
      echo "Aborted."
      exit 5
      ;;
  esac
}

prompt_db_password() {
  local profile="$1"
  local password
  read -r -s -p "Enter destination DB password for profile '$profile': " password
  echo
  if [[ -z "$password" ]]; then
    echo "Error: destination DB password is required." >&2
    exit 3
  fi
  printf '%s' "$password"
}

profile_password_env_name() {
  local profile="$1"
  local upper sanitized
  upper="$(printf '%s' "$profile" | tr '[:lower:]' '[:upper:]')"
  sanitized="$(printf '%s' "$upper" | tr -c 'A-Z0-9' '_')"
  printf '%s_DB_PASS' "$sanitized"
}

get_profile_db_password() {
  local profile="$1"
  local env_name env_value config_value
  env_name="$(profile_password_env_name "$profile")"
  env_value="${!env_name-}"

  if [[ -n "$env_value" ]]; then
    echo "Using destination DB password from \$$env_name for profile '$profile'." >&2
    printf '%s' "$env_value"
    return 0
  fi

  config_value="$(get_profile_value "$profile" password)"
  if [[ -n "$config_value" ]]; then
    echo "Using destination DB password from config profile '$profile'." >&2
    printf '%s' "$config_value"
    return 0
  fi

  prompt_db_password "$profile"
}

print_connectivity_hint() {
  local host="$1"
  if [[ "$host" == "127.0.0.1" || "$host" == "localhost" || "$host" == "::1" ]]; then
    echo "Hint: destination host '$host' is loopback; verify mysql is listening on that interface and port." >&2
  fi
}

connectivity_check() {
  local host="$1"
  local port="$2"
  local user="$3"
  local database="$4"
  local password="$5"

  if ! command -v mysql >/dev/null 2>&1; then
    return 1
  fi

  MYSQL_PWD="$password" mysql -h "$host" -P "$port" -u "$user" -D "$database" -e "SELECT 1;" >/dev/null 2>&1
}

connectivity_check_fast_socket() {
  local host="$1"
  local port="$2"

  if command -v nc >/dev/null 2>&1; then
    nc -z -w 2 "$host" "$port" >/dev/null 2>&1
    return $?
  fi

  if command -v timeout >/dev/null 2>&1; then
    timeout 2 bash -c "exec 3<>/dev/tcp/$host/$port" >/dev/null 2>&1
    return $?
  fi

  # No lightweight socket probe available; treat as pass and rely on mysql for errors.
  return 0
}

mysql_escape_identifier() {
  local ident="$1"
  ident="${ident//\`/\`\`}"
  printf '%s' "$ident"
}

reset_destination_database() {
  local dest_password="$1"
  local dest_host="$2"
  local dest_port="$3"
  local dest_user="$4"
  local dest_db="$5"
  local escaped_db create_stmt reset_sql
  local -a mysql_admin_args=(
    --binary-mode
    --default-character-set=utf8mb4
    --connect-timeout=10
    -h "$dest_host" -P "$dest_port" -u "$dest_user"
  )

  escaped_db="$(mysql_escape_identifier "$dest_db")"
  create_stmt="$(
    MYSQL_PWD="$dest_password" mysql "${mysql_admin_args[@]}" --batch --skip-column-names \
      -e "SHOW CREATE DATABASE \`$escaped_db\`;" 2>/dev/null \
      | awk -F'\t' 'NR == 1 { print $2 }'
  )"

  if [[ -n "$create_stmt" ]]; then
    reset_sql="DROP DATABASE IF EXISTS \`$escaped_db\`; ${create_stmt};"
  else
    reset_sql="DROP DATABASE IF EXISTS \`$escaped_db\`; CREATE DATABASE \`$escaped_db\`;"
  fi

  MYSQL_PWD="$dest_password" mysql "${mysql_admin_args[@]}" -e "$reset_sql"
}

json_get_string_field() {
  local json="$1"
  local field="$2"
  local value=""

  if command -v jq >/dev/null 2>&1; then
    value="$(printf '%s' "$json" | jq -r --arg f "$field" '.[$f] // ""' 2>/dev/null || true)"
    if [[ "$value" != "null" ]]; then
      printf '%s' "$value"
      return
    fi
  fi

  value="$(printf '%s' "$json" | tr -d '\r\n' | sed -n "s/.*\"$field\"[[:space:]]*:[[:space:]]*\"\\([^\"]*\\)\".*/\\1/p")"
  printf '%s' "$value"
}

json_get_number_field() {
  local json="$1"
  local field="$2"
  local value=""

  if command -v jq >/dev/null 2>&1; then
    value="$(printf '%s' "$json" | jq -r --arg f "$field" '.[$f] // ""' 2>/dev/null || true)"
    if [[ "$value" =~ ^[0-9]+$ ]]; then
      printf '%s' "$value"
      return
    fi
  fi

  value="$(printf '%s' "$json" | tr -d '\r\n' | sed -n "s/.*\"$field\"[[:space:]]*:[[:space:]]*\\([0-9][0-9]*\\).*/\\1/p")"
  printf '%s' "$value"
}

mysql_import_from_stdin() {
  local dest_password="$1"
  local dest_host="$2"
  local dest_port="$3"
  local dest_user="$4"
  local dest_db="$5"
  local track_table_progress="$6"
  local table_progress_file="$7"
  local mysql_init_command="SET SESSION sql_mode='NO_AUTO_VALUE_ON_ZERO';SET SESSION foreign_key_checks=0;SET SESSION unique_checks=0"
  local -a mysql_args=(
    --binary-mode
    --default-character-set=utf8mb4
    --max-allowed-packet=1G
    --connect-timeout=10
    --init-command="$mysql_init_command"
    -h "$dest_host" -P "$dest_port" -u "$dest_user" "$dest_db"
  )
  local -a decompress_cmd=(gunzip -c)

  if command -v pigz >/dev/null 2>&1; then
    decompress_cmd=(pigz -dc)
  fi

  if [[ "$track_table_progress" == "true" && -n "$table_progress_file" ]] && command -v awk >/dev/null 2>&1; then
    "${decompress_cmd[@]}" \
      | awk -v status_file="$table_progress_file" '
          function update_table(name) {
            if (name == "" || name == current_table) {
              return
            }
            current_table = name
            print current_table > status_file
            close(status_file)
          }
          function update_from_line(prefix, line, table_name, remainder, tick_pos) {
            if (index(line, prefix) != 1) {
              return 0
            }
            # Keep only the table identifier between prefix and next backtick.
            remainder = substr(line, length(prefix) + 1)
            tick_pos = index(remainder, "`")
            if (tick_pos <= 1) {
              return 0
            }
            table_name = substr(remainder, 1, tick_pos - 1)
            update_table(table_name)
            return 1
          }
          {
            update_from_line("-- Table structure for table `", $0) \
              || update_from_line("-- Dumping data for table `", $0) \
              || update_from_line("CREATE TABLE `", $0) \
              || update_from_line("INSERT INTO `", $0)
            print
          }
        ' \
      | MYSQL_PWD="$dest_password" mysql "${mysql_args[@]}"
    return
  fi

  "${decompress_cmd[@]}" | MYSQL_PWD="$dest_password" mysql "${mysql_args[@]}"
}

run_import() {
  local profile="$DEFAULT_PROFILE"
  local import_arg=""
  local force_local_file="false"
  local no_confirm="false"
  local skip_connectivity_check="false"
  local drop_before_import="true"

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --profile)
        [[ $# -lt 2 ]] && { echo "Error: --profile requires a value" >&2; exit 2; }
        profile="$2"
        shift 2
        ;;
      --id)
        echo "Error: --id is no longer supported." >&2
        echo "Use positional syntax: 'bakker import <id>'." >&2
        exit 2
        ;;
      --file)
        [[ $# -lt 2 ]] && { echo "Error: --file requires a value" >&2; exit 2; }
        if [[ -n "$import_arg" ]]; then
          echo "Error: import accepts exactly one target (<id|filename>)." >&2
          echo "Usage: bakker import [--profile <name>] [--yes] [--skip-connectivity-check] [--no-drop] <id|filename>" >&2
          exit 2
        fi
        import_arg="$2"
        force_local_file="true"
        shift 2
        ;;
      --from-file)
        [[ $# -lt 2 ]] && { echo "Error: --from-file requires a value" >&2; exit 2; }
        if [[ -n "$import_arg" ]]; then
          echo "Error: import accepts exactly one target (<id|filename>)." >&2
          echo "Usage: bakker import [--profile <name>] [--yes] [--skip-connectivity-check] [--no-drop] <id|filename>" >&2
          exit 2
        fi
        import_arg="$2"
        force_local_file="true"
        shift 2
        ;;
      --db|--latest)
        echo "Error: --db/--latest is no longer supported for import." >&2
        echo "Use 'bakker backup list' to find an ID, then run 'bakker import <id>'." >&2
        exit 2
        ;;
      --yes)
        no_confirm="true"
        shift
        ;;
      --skip-connectivity-check)
        skip_connectivity_check="true"
        shift
        ;;
      --no-drop)
        drop_before_import="false"
        shift
        ;;
      -v|--verbose)
        increase_verbosity 1
        shift
        ;;
      -vv)
        increase_verbosity 2
        shift
        ;;
      -vvv)
        increase_verbosity 3
        shift
        ;;
      --help|-h)
        usage_import
        return 0
        ;;
      -*)
        echo "Error: unknown option for import: $1" >&2
        exit 2
        ;;
      *)
        if [[ -n "$import_arg" ]]; then
          echo "Error: import accepts exactly one target (<id|filename>)." >&2
          echo "Usage: bakker import [--profile <name>] [--yes] [--skip-connectivity-check] [--no-drop] <id|filename>" >&2
          exit 2
        fi
        import_arg="$1"
        shift
        ;;
    esac
  done

  [[ -z "$profile" ]] && { echo "Error: --profile is required (or set [defaults].profile / global --profile)" >&2; exit 2; }

  if [[ -z "$import_arg" ]]; then
    echo "Error: import requires an <id|filename> argument." >&2
    echo "Example: bakker import 5" >&2
    echo "Example: bakker import ./dump.sql.gz" >&2
    exit 2
  fi

  require_profile "$profile"

  local host port user database
  host="$(get_profile_value "$profile" host)"
  port="$(get_profile_value "$profile" port)"
  user="$(get_profile_value "$profile" user)"
  database="$(get_profile_value "$profile" database)"

  local filename=""
  local selected_db=""
  local selected_size_bytes=""
  local source_size_bytes=""
  local import_mode=""
  local local_file_path=""
  local token=""
  local run_started_ms resolve_started_ms resolve_ms connectivity_started_ms connectivity_ms
  local import_started_ms import_ms total_ms
  run_started_ms="$(now_millis)"
  resolve_started_ms="$run_started_ms"
  resolve_ms=0
  connectivity_ms=0
  import_ms=0
  total_ms=0
  require_cmd mysql
  require_cmd gunzip
  if [[ "$force_local_file" != "true" && "$import_arg" =~ ^[0-9]+$ ]]; then
    if [[ "$import_arg" -lt 1 ]]; then
      echo "Error: <id> must be a positive integer from 'bakker backup list'." >&2
      exit 2
    fi
    import_mode="id"
    local backup_id="$import_arg"
    token="$(get_auth_token)"
    IFS=$'\t' read -r filename selected_db selected_size_bytes < <(resolve_server_backup_by_id "$backup_id" "$token")
    if [[ "$selected_size_bytes" =~ ^[0-9]+$ && "$selected_size_bytes" -gt 0 ]]; then
      source_size_bytes="$selected_size_bytes"
    fi
    echo "Resolved backup ID $backup_id to '$filename' (database '$selected_db')."
  else
    import_mode="local-file"
    local_file_path="$import_arg"
    if [[ ! -f "$local_file_path" ]]; then
      echo "Error: local file not found: $local_file_path" >&2
      exit 4
    fi
    if [[ "$local_file_path" != *.sql.gz ]]; then
      echo "Error: unsupported local backup format: $local_file_path" >&2
      echo "Expected .sql.gz." >&2
      exit 4
    fi
    filename="$(basename "$local_file_path")"
    source_size_bytes="$(wc -c < "$local_file_path" | tr -d '[:space:]')"
    if [[ ! "$source_size_bytes" =~ ^[0-9]+$ ]]; then
      source_size_bytes=""
    fi
    echo "Using local import file '$local_file_path'."
  fi
  resolve_ms="$(elapsed_millis "$resolve_started_ms")"

  local db_password
  db_password="$(get_profile_db_password "$profile")"

  if [[ "$skip_connectivity_check" == "true" ]]; then
    echo "Running fast destination reachability check for profile '$profile'..." >&2
    connectivity_started_ms="$(now_millis)"
    if ! connectivity_check_fast_socket "$host" "$port"; then
      connectivity_ms="$(elapsed_millis "$connectivity_started_ms")"
      total_ms="$(elapsed_millis "$run_started_ms")"
      log_verbose 1 "Import performance (pre-failure): resolve ${resolve_ms}ms, fast-connectivity ${connectivity_ms}ms, total ${total_ms}ms"
      echo "Error: destination appears unreachable ($host:$port)." >&2
      print_connectivity_hint "$host"
      exit 5
    fi
    connectivity_ms="$(elapsed_millis "$connectivity_started_ms")"
    log_verbose 1 "Skipped MySQL auth preflight; socket reachability check passed."
  else
    echo "Checking destination connectivity for profile '$profile'..."
    connectivity_started_ms="$(now_millis)"
    if ! connectivity_check "$host" "$port" "$user" "$database" "$db_password"; then
      connectivity_ms="$(elapsed_millis "$connectivity_started_ms")"
      total_ms="$(elapsed_millis "$run_started_ms")"
      log_verbose 1 "Import performance (pre-failure): resolve ${resolve_ms}ms, connectivity ${connectivity_ms}ms, total ${total_ms}ms"
      echo "Error: failed to connect to destination DB ($user@$host:$port/$database)." >&2
      print_connectivity_hint "$host"
      exit 5
    fi
    connectivity_ms="$(elapsed_millis "$connectivity_started_ms")"
  fi

  local import_label="$filename"
  if [[ "$import_mode" == "local-file" ]]; then
    import_label="$local_file_path"
  fi
  local confirm_message="Import backup '$import_label' into profile '$profile' ($user@$host:$port/$database)?"
  if [[ "$drop_before_import" == "true" ]]; then
    confirm_message="$confirm_message This will DROP and recreate database '$database' first."
  fi
  confirm_if_needed "$confirm_message" "$no_confirm"
  log_verbose 2 "Import mode: $import_mode"
  log_verbose 2 "Destination: $user@$host:$port/$database"
  if [[ "$import_mode" == "id" ]]; then
    log_verbose 2 "Source: API backup '$filename'"
  else
    log_verbose 2 "Source: local file '$local_file_path'"
  fi
  log_verbose 2 "Execution client: local mysql"
  if [[ "$drop_before_import" == "true" ]]; then
    echo "Resetting destination database '$database' (drop + recreate)..."
    if ! reset_destination_database "$db_password" "$host" "$port" "$user" "$database"; then
      total_ms="$(elapsed_millis "$run_started_ms")"
      log_verbose 1 "Import performance (pre-failure): resolve ${resolve_ms}ms, connectivity ${connectivity_ms}ms, total ${total_ms}ms"
      echo "Error: failed to reset destination database '$database' (drop + recreate)." >&2
      exit 5
    fi
    log_verbose 1 "Destination database reset completed."
  else
    log_verbose 1 "Destination reset disabled (--no-drop)."
  fi
  echo "Starting import of '$import_label' into '$profile'..."

  local progress_interval monitor_pid import_pid import_status started_at now elapsed
  local progress_tmp_dir="" stream_progress_file="" table_progress_file=""
  local track_stream_bytes="false" track_table_progress="false"
  local import_interrupted="false"
  progress_interval="$(import_progress_interval_seconds)"
  started_at="$(date +%s)"

  log_verbose 3 "Verbose level: $VERBOSE_LEVEL (progress heartbeat every ${progress_interval}s)"
  if [[ "$progress_interval" -gt 0 ]]; then
    echo "Import progress updates enabled (every ${progress_interval}s)." >&2
  fi
  if [[ "$VERBOSE_LEVEL" -ge 1 ]]; then
    log_verbose 1 "Verbose levels: -v heartbeat only, -vv adds stream percentage, -vvv adds table tracker."
    if command -v pigz >/dev/null 2>&1; then
      log_verbose 1 "Using decompressor: pigz"
    else
      log_verbose 1 "Using decompressor: gunzip"
    fi
  fi
  if [[ "$VERBOSE_LEVEL" -ge 2 && -n "$source_size_bytes" ]]; then
    if dd_supports_progress; then
      track_stream_bytes="true"
    else
      log_verbose 1 "Detailed stream percentage disabled: your 'dd' does not support status=progress."
    fi
  fi
  if [[ "$VERBOSE_LEVEL" -ge 3 ]]; then
    if command -v awk >/dev/null 2>&1; then
      track_table_progress="true"
    else
      log_verbose 1 "Table-level progress disabled: awk is not available."
    fi
  fi
  if [[ "$track_stream_bytes" == "true" || "$track_table_progress" == "true" ]]; then
    progress_tmp_dir="$(mktemp -d)"
    if [[ "$track_stream_bytes" == "true" ]]; then
      stream_progress_file="$progress_tmp_dir/stream-progress.log"
      : >"$stream_progress_file"
    fi
    if [[ "$track_table_progress" == "true" ]]; then
      table_progress_file="$progress_tmp_dir/current-table.txt"
      : >"$table_progress_file"
    fi
  fi

  trap '
    import_interrupted="true"
    echo "Import interrupted; stopping active transfer..." >&2
    if [[ -n "${monitor_pid:-}" ]]; then
      kill "$monitor_pid" >/dev/null 2>&1 || true
    fi
    if [[ -n "${import_pid:-}" ]]; then
      if command -v pkill >/dev/null 2>&1; then
        pkill -TERM -P "$import_pid" >/dev/null 2>&1 || true
      fi
      kill "$import_pid" >/dev/null 2>&1 || true
    fi
  ' INT TERM

  import_started_ms="$(now_millis)"
  if [[ "$import_mode" == "id" ]]; then
    if [[ "$filename" != *.sql.gz ]]; then
      echo "Error: unsupported server backup format for '$filename'." >&2
      echo "Supported format is .sql.gz." >&2
      exit 4
    fi

    local encoded_filename
    encoded_filename="$(url_encode "$filename")"
    local download_timeout_args=()
    if [[ "$REQUEST_TIMEOUT" != "0" && "$REQUEST_TIMEOUT" != "0.0" ]]; then
      # For large imports, bound connection setup only; do not cap full transfer time.
      download_timeout_args=(--connect-timeout "$REQUEST_TIMEOUT")
    fi

    (
      if [[ "$track_stream_bytes" == "true" ]]; then
        curl -sS --fail-with-body "${download_timeout_args[@]}" \
          -H "Authorization: Bearer $token" \
          "$(normalize_api_url "$BAKKER_API_URL")/api/backups/$encoded_filename" \
          | tee >(dd of=/dev/null status=progress 2>"$stream_progress_file") \
          | mysql_import_from_stdin "$db_password" "$host" "$port" "$user" "$database" "$track_table_progress" "$table_progress_file"
      else
        curl -sS --fail-with-body "${download_timeout_args[@]}" \
          -H "Authorization: Bearer $token" \
          "$(normalize_api_url "$BAKKER_API_URL")/api/backups/$encoded_filename" \
          | mysql_import_from_stdin "$db_password" "$host" "$port" "$user" "$database" "$track_table_progress" "$table_progress_file"
      fi
    ) &
    import_pid=$!
  else
    (
      if [[ "$track_stream_bytes" == "true" ]]; then
        tee >(dd of=/dev/null status=progress 2>"$stream_progress_file") < "$local_file_path" \
          | mysql_import_from_stdin "$db_password" "$host" "$port" "$user" "$database" "$track_table_progress" "$table_progress_file"
      else
        mysql_import_from_stdin "$db_password" "$host" "$port" "$user" "$database" "$track_table_progress" "$table_progress_file" < "$local_file_path"
      fi
    ) &
    import_pid=$!
  fi

  monitor_pid=""
  if [[ "$progress_interval" -gt 0 ]]; then
    (
      previous_transferred_bytes=""
      stalled_seconds=0
      while kill -0 "$import_pid" >/dev/null 2>&1; do
        sleep "$progress_interval"
        if ! kill -0 "$import_pid" >/dev/null 2>&1; then
          break
        fi
        now="$(date +%s)"
        elapsed=$((now - started_at))
        details=""
        if [[ "$VERBOSE_LEVEL" -ge 1 ]]; then
          if [[ "$track_stream_bytes" == "true" && -n "$source_size_bytes" && "$source_size_bytes" -gt 0 ]]; then
            transferred_bytes="$(read_stream_progress_bytes "$stream_progress_file" || true)"
            if [[ "$transferred_bytes" =~ ^[0-9]+$ ]]; then
              percent=$((transferred_bytes * 100 / source_size_bytes))
              if [[ "$percent" -gt 100 ]]; then
                percent=100
              fi
              if [[ "$source_size_bytes" -lt 1048576 ]]; then
                details="stream ${percent}% (${transferred_bytes}/${source_size_bytes} bytes)"
              else
                transferred_mib=$((transferred_bytes / 1024 / 1024))
                total_mib=$((source_size_bytes / 1024 / 1024))
                details="stream ${percent}% (${transferred_mib}/${total_mib} MiB)"
              fi
              if [[ -n "$previous_transferred_bytes" && "$transferred_bytes" -eq "$previous_transferred_bytes" ]]; then
                stalled_seconds=$((stalled_seconds + progress_interval))
              else
                stalled_seconds=0
              fi
              previous_transferred_bytes="$transferred_bytes"
              if [[ "$stalled_seconds" -ge $((progress_interval * 2)) ]]; then
                details="$details, no stream movement for ${stalled_seconds}s"
              fi
            fi
          fi
          if [[ "$track_table_progress" == "true" && -n "$table_progress_file" && -s "$table_progress_file" ]]; then
            current_table="$(tail -n 1 "$table_progress_file" 2>/dev/null | tr -d '\r\n')"
            if [[ -n "$current_table" ]]; then
              if [[ -n "$details" ]]; then
                details="$details, table $current_table"
              else
                details="table $current_table"
              fi
            fi
          fi
        fi
        if [[ -n "$details" ]]; then
          echo "Import in progress (${elapsed}s elapsed, $details)..." >&2
        else
          echo "Import in progress (${elapsed}s elapsed)..." >&2
        fi
      done
    ) &
    monitor_pid=$!
  fi

  set +e
  wait "$import_pid"
  import_status=$?
  set -e

  if [[ -n "$monitor_pid" ]]; then
    kill "$monitor_pid" >/dev/null 2>&1 || true
    wait "$monitor_pid" >/dev/null 2>&1 || true
  fi
  if [[ -n "$progress_tmp_dir" && -d "$progress_tmp_dir" ]]; then
    rm -rf "$progress_tmp_dir"
  fi
  trap - INT TERM
  import_ms="$(elapsed_millis "$import_started_ms")"
  total_ms="$(elapsed_millis "$run_started_ms")"

  if [[ "$import_interrupted" == "true" ]]; then
    log_verbose 1 "Import performance (interrupted): resolve ${resolve_ms}ms, connectivity ${connectivity_ms}ms, stream+load ${import_ms}ms, total ${total_ms}ms"
    echo "Import cancelled."
    exit 130
  fi

  if [[ "$import_status" -ne 0 ]]; then
    log_verbose 1 "Import performance (failure): resolve ${resolve_ms}ms, connectivity ${connectivity_ms}ms, stream+load ${import_ms}ms, total ${total_ms}ms"
    echo "Error: import failed." >&2
    exit "$import_status"
  fi

  if [[ "$VERBOSE_LEVEL" -ge 1 ]]; then
    print_import_perf_summary "${import_mode}/local-mysql" "$resolve_ms" "$connectivity_ms" "$import_ms" "$total_ms" "$source_size_bytes"
  fi
  now="$(date +%s)"
  elapsed=$((now - started_at))
  log_verbose 1 "Import finished in ${import_ms}ms (heartbeat elapsed ${elapsed}s)."
  echo "Import completed successfully."
}

profiles_list() {
  local p
  if [[ -z "$PROFILE_NAMES" ]]; then
    echo "No profiles defined in $CONFIG_PATH"
    return 0
  fi

  for p in $PROFILE_NAMES; do
    printf '%s\n' "$p"
  done
}

profiles_show() {
  local profile="${1:-}"
  [[ -z "$profile" ]] && { echo "Error: profile name is required" >&2; exit 2; }

  require_profile "$profile"

  echo "profile = $profile"
  echo "host = $(get_profile_value "$profile" host)"
  echo "port = $(get_profile_value "$profile" port)"
  echo "user = $(get_profile_value "$profile" user)"
  echo "database = $(get_profile_value "$profile" database)"
  if [[ -n "$(get_profile_value "$profile" password)" ]]; then
    echo "password = [set in config]"
  else
    echo "password = [not set in config]"
  fi
}

doctor() {
  echo "Running doctor checks..."

  require_runtime
  echo "- curl: ok"
  echo "- jq: ok"
  require_cmd mysql
  echo "- mysql: ok"
  require_cmd gunzip
  echo "- gunzip: ok"

  if [[ -f "$CONFIG_PATH" ]]; then
    echo "- config: ok ($CONFIG_PATH)"
  else
    echo "- config: missing ($CONFIG_PATH)"
    exit 3
  fi

  local token
  if [[ -n "${BAKKER_AUTH_TOKEN:-}" ]]; then
    token="$BAKKER_AUTH_TOKEN"
  else
    echo "- token: not set in env (interactive token will be required for API commands)"
    return 0
  fi

  if api_request GET "/api/status" "$token" >/dev/null; then
    echo "- API auth: ok"
  else
    echo "- API auth: failed"
    exit 4
  fi
}

parse_global_options() {
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --config)
        [[ $# -lt 2 ]] && { echo "Error: --config requires a value" >&2; exit 2; }
        CONFIG_PATH="$2"
        shift 2
        ;;
      --api-url)
        [[ $# -lt 2 ]] && { echo "Error: --api-url requires a value" >&2; exit 2; }
        OVERRIDE_API_URL="$2"
        shift 2
        ;;
      --timeout)
        [[ $# -lt 2 ]] && { echo "Error: --timeout requires a value" >&2; exit 2; }
        OVERRIDE_TIMEOUT="$2"
        shift 2
        ;;
      --output)
        [[ $# -lt 2 ]] && { echo "Error: --output requires a value" >&2; exit 2; }
        OVERRIDE_OUTPUT="$2"
        shift 2
        ;;
      --profile)
        [[ $# -lt 2 ]] && { echo "Error: --profile requires a value" >&2; exit 2; }
        OVERRIDE_PROFILE="$2"
        shift 2
        ;;
      -v|--verbose)
        increase_verbosity 1
        shift
        ;;
      -vv)
        increase_verbosity 2
        shift
        ;;
      -vvv)
        increase_verbosity 3
        shift
        ;;
      --help|-h)
        usage
        exit 0
        ;;
      help)
        shift
        command_help "${1:-}"
        exit 0
        ;;
      version)
        echo "$VERSION"
        exit 0
        ;;
      backup|backups|import|profiles|doctor)
        break
        ;;
      *)
        echo "Error: unknown global option or command: $1" >&2
        usage >&2
        exit 2
        ;;
    esac
  done

  REMAINING_ARGS=("$@")
}

main() {
  require_runtime
  parse_global_options "$@"
  set -- "${REMAINING_ARGS[@]}"

  if [[ $# -lt 1 ]]; then
    usage
    exit 2
  fi

  local cmd="$1"
  shift

  # Help paths should work even when config file is missing.
  case "$cmd" in
    backup|backups)
      if [[ "${1:-}" == "--help" || "${1:-}" == "-h" || "${1:-}" == "help" || -z "${1:-}" ]]; then
        usage_backups
        return 0
      fi
      if [[ "${1:-}" == "list" && ( "${2:-}" == "--help" || "${2:-}" == "-h" ) ]]; then
        usage_backups
        return 0
      fi
      if [[ "${1:-}" == "download" && ( "${2:-}" == "--help" || "${2:-}" == "-h" ) ]]; then
        usage_backup_download
        return 0
      fi
      ;;
    import)
      if [[ "${1:-}" == "--help" || "${1:-}" == "-h" || "${1:-}" == "help" ]]; then
        usage_import
        return 0
      fi
      ;;
    profiles)
      if [[ "${1:-}" == "--help" || "${1:-}" == "-h" || "${1:-}" == "help" || -z "${1:-}" ]]; then
        usage_profiles
        return 0
      fi
      ;;
    doctor)
      if [[ "${1:-}" == "--help" || "${1:-}" == "-h" || "${1:-}" == "help" ]]; then
        usage_doctor
        return 0
      fi
      ;;
  esac

  load_config

  case "$cmd" in
    backup|backups)
      local sub="${1:-}"
      shift || true
      case "$sub" in
        list)
          command_backups_list "$@"
          ;;
        download)
          command_backups_download "$@"
          ;;
        *)
          echo "Error: expected 'backup list' or 'backup download'" >&2
          exit 2
          ;;
      esac
      ;;
    import)
      run_import "$@"
      ;;
    profiles)
      local sub="${1:-}"
      shift || true
      case "$sub" in
        list) profiles_list ;;
        show) profiles_show "${1:-}" ;;
        *)
          echo "Error: profiles command must be 'list' or 'show <name>'" >&2
          exit 2
          ;;
      esac
      ;;
    doctor)
      doctor
      ;;
    *)
      echo "Error: unknown command: $cmd" >&2
      usage >&2
      exit 2
      ;;
  esac
}

main "$@"
